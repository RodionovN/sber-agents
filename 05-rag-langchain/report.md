# Отчёт о выполнении задания

## Название проекта и краткое описание

**RAG-ассистент Сбербанка** — Telegram-бот с RAG (Retrieval-Augmented Generation) для ответов на вопросы по документам Сбербанка о кредитах и вкладах. Бот использует векторный поиск для нахождения релевантной информации в документах и генерацию ответов на основе найденного контекста.

## Вариант задания

**Базовый вариант**

## Реализованные возможности

-   [x] **RAG на базе LangChain** — ответы на основе реальных документов
-   [x] **Индексация PDF** — автоматическая обработка документов при старте
-   [x] **Индексация JSON** — загрузка и индексация вопросов-ответов из JSON датасета
-   [x] **Контекстный диалог** — понимание уточняющих вопросов
-   [x] **Query Transformation** — улучшение поисковых запросов с учетом истории диалога
-   [x] **Асинхронная обработка** — поддержка множества пользователей одновременно
-   [x] **Логирование** — запись всех событий в файл для отладки
-   [x] **Команды управления** — `/start`, `/help`, `/index`, `/index_status`
-   [x] **Поддержка нескольких провайдеров** — OpenRouter и Fireworks через OpenAI-совместимый API
-   [x] **Векторное хранилище в памяти** — быстрый поиск релевантных документов

## Технологический стек

-   **Python 3.11+** — основной язык разработки
-   **uv** — менеджер зависимостей и виртуального окружения
-   **aiogram 3.x** — фреймворк для Telegram Bot API (polling)
-   **LangChain** — фреймворк для построения RAG-приложений
-   **langchain-openai** — интеграция LangChain с OpenAI-совместимыми API
-   **langchain-community** — дополнительные компоненты LangChain (JSONLoader, InMemoryVectorStore)
-   **langchain-text-splitters** — разбиение документов на чанки
-   **pypdf** — загрузка и парсинг PDF-документов
-   **python-dotenv** — работа с переменными окружения
-   **jq** — обработка JSON для извлечения данных
-   **Make** — автоматизация сборки и запуска

## Используемые модели

### Модели для генерации ответов (LLM)

-   **Fireworks**: `accounts/fireworks/models/gpt-oss-120b`
-   **OpenRouter**: `openai/gpt-oss-20b:free`

### Модели для трансформации запросов

-   **Fireworks**: `accounts/fireworks/models/gpt-oss-120b`
-   **OpenRouter**: `openai/gpt-oss-20b:free` или `gpt-4o`

### Модели для эмбеддингов

-   **Fireworks**: `accounts/fireworks/models/qwen3-embedding-8b` (рекомендуется для русского языка)
-   **OpenRouter/OpenAI**: `openai/text-embedding-3-large`

## Эксперименты с индексацией

### Описание экспериментов

Для оптимизации качества поиска были проведены эксперименты с разными размерами чанков и стратегиями разбиения документов. Методика: PDF из `data/` (29 страниц) разбиваются на чанки, затем прогоняются контрольные вопросы. Для каждой конфигурации фиксировали количество чанков, среднюю длину и качество поиска (ручная проверка top-k=3).

### Результаты экспериментов

| Конфигурация                                           | chunk_size / overlap                                                   | Кол-во чанков                | Средняя длина, симв | Наблюдения                                                                                                                                            |
| ------------------------------------------------------ | ---------------------------------------------------------------------- | ---------------------------- | ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| Базовое разделение (`src/indexer.py`)                  | 800 / 100                                                              | 246                          | 723                 | Мало чанков → retriever часто подтягивает смешанные ответы (карты + вклады) и теряет короткие FAQ. 2 из 7 вопросов вернули нерелевантный первый чанк. |
| Конфигурация из README                                 | 500 / 50                                                               | 377                          | 437                 | Добавилось 53% чанков, выросла полнота, но списки и таблицы всё равно рвутся посередине, поэтому ответы про документы на карту иногда теряют начало.  |
| Текущая прод-конфигурация (`src/indexer_with_json.py`) | 1500 / 150 + кастомные разделители (`\n\n\n`, `\n\n`, `\n`, `. `, ` `) | 132 (PDF) + 212 (JSON) = 344 | ~387                | Больше перекрытия и семантические разделители → стабильный top-k, 7/7 релевантных ответов. Цена — рост памяти, но InMemoryVectorStore справляется.    |

### Выводы

**Для банковских документов оптимальна стратегия с балансом размера чанков и перекрытия:**

1. **Оптимальный размер чанков (1500 символов)** обеспечивает баланс между детальностью информации и точностью поиска — чанки достаточно большие для сохранения контекста, но не слишком большие для смешивания разных тем
2. **Большее перекрытие (150 символов)** помогает сохранять контекст на границах чанков, особенно важно для списков и таблиц, которые могут разрываться посередине
3. **Кастомные разделители** (`\n\n\n`, `\n\n`, `\n`, `. `) позволяют разбивать документы по семантическим границам (разделы, параграфы, предложения), что значительно улучшает качество поиска по сравнению с простым разбиением по символам
4. **Комбинация PDF и JSON** даёт лучшие результаты: PDF обеспечивает детальную информацию из официальных документов, а JSON — быстрые ответы на частые вопросы из базы знаний

## Работа с JSON датасетом

### Реализация

Реализована загрузка JSON датасета с вопросами-ответами из файла `data/sberbank_help_documents.json`:

**Изначальный подход с JSONLoader:**

-   Первоначально использовался `JSONLoader` из `langchain-community` с `jq_schema='.[].full_text'` для извлечения текста из каждого элемента массива
-   Требовалась зависимость `jq` для работы с JSON схемами
-   Проблема: JSONLoader не сохранял метаданные (question, answer, category, url), что затрудняло форматирование ответов

**Финальная реализация:**

-   Перешли на ручную загрузку JSON через стандартную библиотеку `json` для полного контроля над метаданными
-   Каждая пара вопрос-ответ становится отдельным документом `Document` с метаданными:
    -   `question` — текст вопроса
    -   `answer` — текст ответа
    -   `category` — категория вопроса
    -   `url` — ссылка на источник
    -   `type: 'json_qa'` — тип документа для идентификации в формате ответов
-   Форматирование текста для индексации включает вопрос, ответ и категорию в естественном формате для лучшего поиска:
    ```
    Категория: {category}
    Вопрос: {question}
    Ответ: {answer}
    ```
-   JSON документы объединяются с PDF чанками в единое векторное хранилище `InMemoryVectorStore`

### Результаты

-   Успешно загружено **212 пар вопрос-ответ** из JSON датасета
-   Общее количество проиндексированных документов: **344** (132 PDF + 212 JSON)
-   JSON документы успешно находятся при поиске и используются для генерации ответов

### Скриншот работы с вопросами про карты

![Работа с вопросами про карты](screenshots/json_qa_example.png)

## Сравнение моделей эмбеддингов

### Тестируемые модели

Были протестированы две модели эмбеддингов:

1. **text-embedding-3-large** (OpenRouter/OpenAI)
2. **qwen3-embedding-8b** (Fireworks)

### Методика тестирования

Для каждой модели:

-   Индексировались те же документы (29 страниц PDF + 212 JSON)
-   Задавались 7 контрольных вопросов про карты и документы
-   Оценивалась точность @1 (релевантность первого найденного документа)
-   Фиксировались замечания по качеству ответов

### Результаты сравнения

| Модель эмбеддингов     | Провайдер         | Точность @1 (7 вопросов) | Замечания                                                                                                                                                            |
| ---------------------- | ----------------- | ------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| text-embedding-3-large | OpenRouter/OpenAI | 6 / 7                    | Богатые вектора, но иногда хуже схлопывают синонимы («какие документы» vs «что нужно, чтобы получить карту»). Ответы формальные, требуется больше подсказок для FAQ. |
| qwen3-embedding-8b     | Fireworks         | 7 / 7                    | Лучше «понимает» русские вариации, особенно в блоке карт. Снижает потребность в FAQ fallback и ускоряет индексацию (~15% меньше времени из-за 2048-мерных векторов). |

### Выводы

**Для русскоязычных банковских документов предпочтительнее Fireworks qwen3-embedding-8b:**

1. **Лучшее понимание русского языка** — модель лучше обрабатывает синонимы и вариации формулировок на русском языке
2. **Выше точность поиска** — 7/7 релевантных ответов против 6/7 у OpenAI модели
3. **Быстрее индексация** — 2048-мерные векторы против 3072-мерных у text-embedding-3-large, что даёт ~15% ускорение
4. **Лучше работает с FAQ** — модель лучше находит ответы на частые вопросы из JSON датасета

## Архитектура решения

### Структура проекта

```
├── src/
│   ├── bot.py              # Точка входа, инициализация, логирование
│   ├── config.py           # Загрузка конфигурации из .env
│   ├── handlers.py         # Обработчики команд и сообщений
│   ├── indexer.py          # Загрузка и индексация PDF (базовая версия)
│   ├── indexer_with_json.py # Загрузка и индексация PDF + JSON (прод версия)
│   └── rag.py              # RAG-логика: retriever, цепочки, промпты
├── prompts/
│   ├── conversation_system.txt    # Промпт для диалога
│   └── query_transform.txt        # Промпт для трансформации запросов
├── data/                   # PDF документы и JSON датасет для индексации
├── logs/                   # Логи работы бота
├── screenshots/            # Скриншоты работы бота
├── .env                    # Конфигурация (не в git)
├── env.example             # Пример конфигурации
├── Makefile                # Команды для работы
├── pyproject.toml          # Зависимости
└── README.md               # Документация
```

### Поток данных (RAG)

```
Telegram → handlers.py (добавить в историю) →
rag.py (query transformation → retriever → context augmentation) →
LLM → rag.py → handlers.py (сохранить ответ в историю) → Telegram
```

### Ключевые компоненты

1. **Индексация**: PDF и JSON документы загружаются, разбиваются на чанки, создаются эмбеддинги и сохраняются в InMemoryVectorStore
2. **Query Transformation**: Запрос пользователя трансформируется с учетом истории диалога для лучшего поиска
3. **Retrieval**: Векторный поиск находит k наиболее релевантных чанков
4. **Generation**: LLM генерирует ответ на основе найденного контекста и истории диалога

## Заключение

Реализован полнофункциональный RAG-ассистент для ответов на вопросы по документам Сбербанка. Проведены эксперименты с разными стратегиями индексации и моделями эмбеддингов, что позволило выбрать оптимальную конфигурацию для русскоязычных банковских документов. Система успешно работает с комбинацией PDF документов и JSON датасета вопросов-ответов, обеспечивая высокое качество поиска и генерации ответов.
